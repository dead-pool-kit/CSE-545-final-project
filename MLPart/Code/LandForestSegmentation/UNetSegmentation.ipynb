{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UNetSegmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["####################################################################################################\n","\n","Team Members : Pulkit Varshney, Pratik Thorwe, Purva Makarand Mhasakar, Saurabh Parekh\n","\n","Code File Description : This code uses tensorflow approach to train a Unet model based on the Semantic segmentation of aerial imagery dataset. Once the model is trained, we use to predict the segmentation on the satellite images of the Multi-Temporal Urban Development SpaceNet Dataset.\n","\n","####################################################################################################"],"metadata":{"id":"DbVzFOnfYz0e"}},{"cell_type":"markdown","source":["Code base has been referred and built on top of : \n","\n","Source :\n","1. https://towardsdatascience.com/u-net-for-semantic-segmentation-on-unbalanced-aerial-imagery-3474fa1d3e56\n","\n","2. https://github.com/amirhosseinh77/UNet-AerialSegmentation"],"metadata":{"id":"wyh_6247cBUk"}},{"cell_type":"code","source":["import cv2\n","import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import PIL\n","import random\n","from scipy import ndimage\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from random import shuffle\n","from torch import nn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from glob import glob\n","import sys\n","import shutil  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","%matplotlib inline"],"metadata":{"id":"8v10XULDYeYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6nhF1um1VBZ","executionInfo":{"status":"ok","timestamp":1652579559800,"user_tz":240,"elapsed":125,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["class segmentClasses(torch.utils.data.Dataset):\n","    def __init__(self, root, training, transform=None):\n","        super(segmentClasses, self).__init__()\n","        self.root = root\n","        self.training = training\n","        self.transform = transform\n","        self.IMG_NAMES = sorted(glob(self.root + '/*/images/*.jpg'))\n","        self.BGR_classes = {'Water' : [ 41, 169, 226],\n","                            'Land' : [246,  41, 132],\n","                            'Road' : [228, 193, 110],\n","                            'Building' : [152,  16,  60], \n","                            'Vegetation' : [ 58, 221, 254],\n","                            'Unlabeled' : [155, 155, 155]} # in BGR\n","\n","        self.bin_classes = ['Water', 'Land', 'Road', 'Building', 'Vegetation', 'Unlabeled']\n","\n","\n","    def __getitem__(self, idx):\n","        img_path = self.IMG_NAMES[idx]\n","        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n","\n","        image = cv2.imread(img_path)\n","        mask = cv2.imread(mask_path)\n","        cls_mask = np.zeros(mask.shape)  \n","        cls_mask[mask == self.BGR_classes['Water']] = self.bin_classes.index('Water')\n","        cls_mask[mask == self.BGR_classes['Land']] = self.bin_classes.index('Land')\n","        cls_mask[mask == self.BGR_classes['Road']] = self.bin_classes.index('Road')\n","        cls_mask[mask == self.BGR_classes['Building']] = self.bin_classes.index('Building')\n","        cls_mask[mask == self.BGR_classes['Vegetation']] = self.bin_classes.index('Vegetation')\n","        cls_mask[mask == self.BGR_classes['Unlabeled']] = self.bin_classes.index('Unlabeled')\n","        cls_mask = cls_mask[:,:,0] \n","\n","        if self.training==True:\n","            if self.transform:\n","              image = transforms.functional.to_pil_image(image)\n","              image = self.transform(image)\n","              image = np.array(image)\n","\n","        image = cv2.resize(image, (512,512))/255.0\n","        cls_mask = cv2.resize(cls_mask, (512,512)) \n","        image = np.moveaxis(image, -1, 0)\n","\n","        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64)\n","\n","\n","    def __len__(self):\n","        return len(self.IMG_NAMES)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"VCNI4Ap01alL","executionInfo":{"status":"ok","timestamp":1652579567828,"user_tz":240,"elapsed":129,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["color_shift = transforms.ColorJitter(.1,.1,.1,.1)\n","blurriness = transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n","t = transforms.Compose([color_shift, blurriness])\n","dataset = segmentClasses('./Semantic segmentation dataset', training = True, transform= t)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlpEdstKPyqg","executionInfo":{"status":"ok","timestamp":1652579572450,"user_tz":240,"elapsed":129,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["test_num = int(0.1 * len(dataset))\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset)-test_num, test_num], generator=torch.Generator().manual_seed(101))"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1_T8eQ51iRn","outputId":"de77a014-0893-4851-963e-606e90cfa22d","executionInfo":{"status":"ok","timestamp":1652579574480,"user_tz":240,"elapsed":147,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["BACH_SIZE = 4\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=4)\n","\n","test_dataloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=BACH_SIZE, shuffle=False, num_workers=1)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"markdown","source":["Source :\n","1. https://towardsdatascience.com/u-net-for-semantic-segmentation-on-unbalanced-aerial-imagery-3474fa1d3e56\n","\n","2. https://github.com/amirhosseinh77/UNet-AerialSegmentation"],"metadata":{"id":"uii2m7x5cafS"}},{"cell_type":"code","metadata":{"id":"uaugx4aaO9sR","executionInfo":{"status":"ok","timestamp":1652579576242,"user_tz":240,"elapsed":131,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","\n","        # if bilinear, use the normal convolutions to reduce the number of channels\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"APxNt4tuQako","executionInfo":{"status":"ok","timestamp":1652579577890,"user_tz":240,"elapsed":169,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=True):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        factor = 2 if bilinear else 1\n","        self.down4 = Down(512, 1024 // factor)\n","        self.up1 = Up(1024, 512 // factor, bilinear)\n","        self.up2 = Up(512, 256 // factor, bilinear)\n","        self.up3 = Up(256, 128 // factor, bilinear)\n","        self.up4 = Up(128, 64, bilinear)\n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vywh2VYYSQpM","executionInfo":{"status":"ok","timestamp":1652579594040,"user_tz":240,"elapsed":127,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["class mIoULoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, n_classes=2):\n","        super(mIoULoss, self).__init__()\n","        self.classes = n_classes\n","\n","    def to_one_hot(self, tensor):\n","        n,h,w = tensor.size()\n","        one_hot = torch.zeros(n,self.classes,h,w).to(tensor.device).scatter_(1,tensor.view(n,1,h,w),1)\n","        return one_hot\n","\n","    def forward(self, inputs, target):\n","        # inputs => N x Classes x H x W\n","        # target_oneHot => N x Classes x H x W\n","        N = inputs.size()[0]\n","        # predicted probabilities for each pixel along channel\n","        inputs = F.softmax(inputs,dim=1)\n","        # Numerator Product\n","        target_oneHot = self.to_one_hot(target)\n","        inter = inputs * target_oneHot\n","        ## Sum over all pixels N x C x H x W => N x C\n","        inter = inter.view(N,self.classes,-1).sum(2)\n","        #Denominator \n","        union= inputs + target_oneHot - (inputs*target_oneHot)\n","        ## Sum over all pixels N x C x H x W => N x C\n","        union = union.view(N,self.classes,-1).sum(2)\n","        loss = inter/union\n","        ## Return average loss over classes and batch\n","        return 1-loss.mean()"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FTvBfpMSTR9","executionInfo":{"status":"ok","timestamp":1652579594848,"user_tz":240,"elapsed":177,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["criterion = mIoULoss(n_classes=6).to(device)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZqa0-aofjge","executionInfo":{"status":"ok","timestamp":1652579596753,"user_tz":240,"elapsed":144,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["def acc(label, predicted):\n","  seg_acc = (y.cpu() == torch.argmax(pred_mask, axis=1).cpu()).sum() / torch.numel(y.cpu())\n","  return seg_acc"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"2URZk5o240xZ","executionInfo":{"status":"ok","timestamp":1652579597768,"user_tz":240,"elapsed":363,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}}},"source":["min_loss = torch.tensor(float('inf'))\n","\n","model = UNet(n_channels=3, n_classes=6, bilinear=True).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"],"execution_count":28,"outputs":[]},{"cell_type":"code","source":["os.makedirs('./saved_models', exist_ok=True)\n","\n","N_EPOCHS = 30\n","N_DATA = len(train_dataset)\n","N_TEST = len(test_dataset)\n","\n","plot_losses = []\n","scheduler_counter = 0\n","\n","for epoch in range(N_EPOCHS):\n","  # training\n","  model.train()\n","  loss_list = []\n","  acc_list = []\n","  for batch_i, (x, y) in enumerate(train_dataloader):\n","\n","      pred_mask = model(x.to(device))  \n","      loss = criterion(pred_mask, y.to(device))\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      loss_list.append(loss.cpu().detach().numpy())\n","      acc_list.append(acc(y,pred_mask).numpy())\n","\n","  scheduler_counter += 1\n","  # testing\n","  model.eval()\n","  val_loss_list = []\n","  val_acc_list = []\n","  for batch_i, (x, y) in enumerate(test_dataloader):\n","      with torch.no_grad():    \n","          pred_mask = model(x.to(device))  \n","      val_loss = criterion(pred_mask, y.to(device))\n","      val_loss_list.append(val_loss.cpu().detach().numpy())\n","      val_acc_list.append(acc(y,pred_mask).numpy())\n","    \n","  print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n","                                                                                                 np.mean(loss_list), \n","                                                                                                 np.mean(acc_list), \n","                                                                                                 np.mean(val_loss_list),\n","                                                                                                 np.mean(val_acc_list)))\n","  plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n","\n","  compare_loss = np.mean(val_loss_list)\n","  is_best = compare_loss < min_loss\n","  if is_best == True:\n","    scheduler_counter = 0\n","    min_loss = min(compare_loss, min_loss)\n","    torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n","  \n","  if scheduler_counter > 5:\n","    lr_scheduler.step()\n","    print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n","    scheduler_counter = 0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0txIZNhVBZYs","executionInfo":{"status":"ok","timestamp":1652580898561,"user_tz":240,"elapsed":1268393,"user":{"displayName":"Purva Mhasakar","userId":"09757849624704371253"}},"outputId":"72e35bae-a1c7-4802-d9c8-cd53a41c2770"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":[" epoch 0 - loss : 0.84988 - acc : 0.57 - val loss : 0.87128 - val acc : 0.56\n"," epoch 1 - loss : 0.78913 - acc : 0.66 - val loss : 0.85466 - val acc : 0.52\n"," epoch 2 - loss : 0.76463 - acc : 0.67 - val loss : 0.76258 - val acc : 0.68\n"," epoch 3 - loss : 0.74589 - acc : 0.67 - val loss : 0.80282 - val acc : 0.62\n"," epoch 4 - loss : 0.73538 - acc : 0.68 - val loss : 0.73529 - val acc : 0.70\n"," epoch 5 - loss : 0.74284 - acc : 0.66 - val loss : 0.73029 - val acc : 0.66\n"," epoch 6 - loss : 0.71064 - acc : 0.70 - val loss : 0.69829 - val acc : 0.74\n"," epoch 7 - loss : 0.70885 - acc : 0.68 - val loss : 0.71434 - val acc : 0.72\n"," epoch 8 - loss : 0.70090 - acc : 0.71 - val loss : 0.71199 - val acc : 0.72\n"," epoch 9 - loss : 0.70510 - acc : 0.71 - val loss : 0.72309 - val acc : 0.71\n"," epoch 10 - loss : 0.70406 - acc : 0.69 - val loss : 0.69447 - val acc : 0.74\n"," epoch 11 - loss : 0.68878 - acc : 0.72 - val loss : 0.83339 - val acc : 0.30\n"," epoch 12 - loss : 0.70709 - acc : 0.70 - val loss : 0.69533 - val acc : 0.69\n"," epoch 13 - loss : 0.69733 - acc : 0.71 - val loss : 0.68332 - val acc : 0.72\n"," epoch 14 - loss : 0.69405 - acc : 0.71 - val loss : 0.67827 - val acc : 0.76\n"," epoch 15 - loss : 0.68977 - acc : 0.72 - val loss : 0.67122 - val acc : 0.72\n"," epoch 16 - loss : 0.69365 - acc : 0.72 - val loss : 0.68978 - val acc : 0.71\n"," epoch 17 - loss : 0.69134 - acc : 0.71 - val loss : 0.73967 - val acc : 0.56\n"," epoch 18 - loss : 0.69333 - acc : 0.71 - val loss : 0.67377 - val acc : 0.75\n"," epoch 19 - loss : 0.67969 - acc : 0.72 - val loss : 0.67413 - val acc : 0.75\n"," epoch 20 - loss : 0.68828 - acc : 0.71 - val loss : 0.68488 - val acc : 0.73\n"," epoch 21 - loss : 0.67419 - acc : 0.73 - val loss : 0.68663 - val acc : 0.74\n","lowering learning rate to 0.0005\n"," epoch 22 - loss : 0.68268 - acc : 0.72 - val loss : 0.67872 - val acc : 0.74\n"," epoch 23 - loss : 0.66430 - acc : 0.76 - val loss : 0.66754 - val acc : 0.75\n"," epoch 24 - loss : 0.67249 - acc : 0.73 - val loss : 0.65754 - val acc : 0.76\n"," epoch 25 - loss : 0.67886 - acc : 0.73 - val loss : 0.68227 - val acc : 0.73\n"," epoch 26 - loss : 0.67490 - acc : 0.73 - val loss : 0.66948 - val acc : 0.75\n"," epoch 27 - loss : 0.67684 - acc : 0.72 - val loss : 0.65264 - val acc : 0.78\n"," epoch 28 - loss : 0.66486 - acc : 0.75 - val loss : 0.66722 - val acc : 0.74\n"," epoch 29 - loss : 0.66129 - acc : 0.75 - val loss : 0.67943 - val acc : 0.74\n"]}]},{"cell_type":"code","metadata":{"id":"aH31qRzp6xep"},"source":["model.load_state_dict(torch.load('/content/saved_models/unet_epoch_27_0.65264.pt'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["######################################################################################################\n","\n","On the trained model, satellite images of SpaceNet dataset\n","are passed for prediction. Based on the output segmented graph,\n","land area and green area percentage coverage is obtained. The outputs are written to csv files.\n","#######################################################################################################"],"metadata":{"id":"gZ_3LGGnh3X7"}},{"cell_type":"code","source":["import cv2 as cv\n","import matplotlib.image as img\n","from google.colab.patches import cv2_imshow\n","import csv\n","count=0\n","data=[]\n","for filename in os.listdir('/content/satelliteImgs/images/'):\n","        # print(filename)\n","        fn='/content/satelliteImgs/images/'+filename\n","        test_img = cv2.imread(fn)\n","        test_img = cv2.resize(test_img, (256,256))/255.0\n","        test_img = np.moveaxis(test_img, -1, 0).reshape(-1,3,256,256)\n","        test_img = torch.tensor(test_img).float()\n","        result = model(test_img.to(device))\n","        mask = torch.argmax(result, axis=1).cpu().detach().numpy()[0] \n","        count=count+1\n","        fName='/content/masked'+str(count)+'.png'\n","        plt.imsave(fName, mask/255.0)\n","        image = img.imread(fName)\n","        hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)#converting to HSV image\n","        yellow_mask = cv.inRange(hsv, (61,41,133), (120,255,255))\n","        green_mask = cv.inRange(hsv, (60, 0, 0), (92, 255,255))\n","        blue_mask = cv.inRange(hsv, (100, 0, 0), (110, 255,255))\n","        percentage = lambda mask: round(((mask > 0).mean()) * 100,3)\n","        greenery_percentage = percentage(yellow_mask)\n","        land_percentage = percentage(green_mask) + percentage(blue_mask)\n","        fnfinal=filename.replace('.png','.tif')\n","        data.append([fnfinal,greenery_percentage,land_percentage])\n","with open('/content/satelliteImgs/coverPercentage.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerows(data)"],"metadata":{"id":"0fHoGQbdJbIZ"},"execution_count":null,"outputs":[]}]}